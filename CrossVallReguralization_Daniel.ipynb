{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X30</th>\n",
       "      <th>X31</th>\n",
       "      <th>X32</th>\n",
       "      <th>X33</th>\n",
       "      <th>X34</th>\n",
       "      <th>X35</th>\n",
       "      <th>X36</th>\n",
       "      <th>X37</th>\n",
       "      <th>X38</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.00000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1.029000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-12.492680</td>\n",
       "      <td>327718.372035</td>\n",
       "      <td>2.565422</td>\n",
       "      <td>-0.018122</td>\n",
       "      <td>1.235180</td>\n",
       "      <td>-153.782162</td>\n",
       "      <td>-23648.95337</td>\n",
       "      <td>-2.565422</td>\n",
       "      <td>0.018122</td>\n",
       "      <td>-4.108330</td>\n",
       "      <td>...</td>\n",
       "      <td>5.612245</td>\n",
       "      <td>100.650146</td>\n",
       "      <td>-29.719145</td>\n",
       "      <td>-136.164237</td>\n",
       "      <td>-101.649174</td>\n",
       "      <td>-34.310010</td>\n",
       "      <td>-7.965986</td>\n",
       "      <td>-37.117590</td>\n",
       "      <td>2.499514</td>\n",
       "      <td>-8.035132e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>572.608894</td>\n",
       "      <td>298395.344783</td>\n",
       "      <td>0.424105</td>\n",
       "      <td>0.715733</td>\n",
       "      <td>58.709228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.424105</td>\n",
       "      <td>0.715733</td>\n",
       "      <td>584.648570</td>\n",
       "      <td>...</td>\n",
       "      <td>239.009325</td>\n",
       "      <td>2862.331915</td>\n",
       "      <td>4489.803784</td>\n",
       "      <td>2641.944700</td>\n",
       "      <td>5153.481524</td>\n",
       "      <td>2569.220416</td>\n",
       "      <td>2933.878621</td>\n",
       "      <td>1504.903744</td>\n",
       "      <td>102.994972</td>\n",
       "      <td>2.593868e+20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-992.795572</td>\n",
       "      <td>1.098476</td>\n",
       "      <td>0.020395</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-153.782162</td>\n",
       "      <td>-23648.95337</td>\n",
       "      <td>-2.999260</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-996.652132</td>\n",
       "      <td>...</td>\n",
       "      <td>-410.000000</td>\n",
       "      <td>-4857.000000</td>\n",
       "      <td>-7715.000000</td>\n",
       "      <td>-4596.000000</td>\n",
       "      <td>-8898.000000</td>\n",
       "      <td>-4383.000000</td>\n",
       "      <td>-5133.000000</td>\n",
       "      <td>-2565.000000</td>\n",
       "      <td>-175.000000</td>\n",
       "      <td>-9.506460e+20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-499.000060</td>\n",
       "      <td>59791.243170</td>\n",
       "      <td>2.388319</td>\n",
       "      <td>-0.760277</td>\n",
       "      <td>-51.000000</td>\n",
       "      <td>-153.782162</td>\n",
       "      <td>-23648.95337</td>\n",
       "      <td>-2.869121</td>\n",
       "      <td>-0.714801</td>\n",
       "      <td>-511.919891</td>\n",
       "      <td>...</td>\n",
       "      <td>-189.000000</td>\n",
       "      <td>-2457.000000</td>\n",
       "      <td>-4063.000000</td>\n",
       "      <td>-2438.000000</td>\n",
       "      <td>-4780.000000</td>\n",
       "      <td>-2311.000000</td>\n",
       "      <td>-2520.000000</td>\n",
       "      <td>-1411.000000</td>\n",
       "      <td>-85.000000</td>\n",
       "      <td>-7.703790e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-27.174576</td>\n",
       "      <td>241226.533900</td>\n",
       "      <td>2.691213</td>\n",
       "      <td>-0.029038</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-153.782162</td>\n",
       "      <td>-23648.95337</td>\n",
       "      <td>-2.691213</td>\n",
       "      <td>0.029038</td>\n",
       "      <td>-6.507760</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>-196.000000</td>\n",
       "      <td>-13.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>-35.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-1.094503e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>477.216100</td>\n",
       "      <td>547320.664300</td>\n",
       "      <td>2.869121</td>\n",
       "      <td>0.714801</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>-153.782162</td>\n",
       "      <td>-23648.95337</td>\n",
       "      <td>-2.388319</td>\n",
       "      <td>0.760277</td>\n",
       "      <td>516.046297</td>\n",
       "      <td>...</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>2654.000000</td>\n",
       "      <td>3890.000000</td>\n",
       "      <td>2138.000000</td>\n",
       "      <td>4195.000000</td>\n",
       "      <td>2165.000000</td>\n",
       "      <td>2521.000000</td>\n",
       "      <td>1227.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>5.636450e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>998.297367</td>\n",
       "      <td>996597.632200</td>\n",
       "      <td>2.999260</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-153.782162</td>\n",
       "      <td>-23648.95337</td>\n",
       "      <td>-0.020395</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>1000.933758</td>\n",
       "      <td>...</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>4862.000000</td>\n",
       "      <td>7752.000000</td>\n",
       "      <td>4577.000000</td>\n",
       "      <td>8866.000000</td>\n",
       "      <td>4384.000000</td>\n",
       "      <td>5123.000000</td>\n",
       "      <td>2568.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>9.881420e+20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                X1             X2           X3           X4           X5  \\\n",
       "count  1029.000000    1029.000000  1029.000000  1029.000000  1029.000000   \n",
       "mean    -12.492680  327718.372035     2.565422    -0.018122     1.235180   \n",
       "std     572.608894  298395.344783     0.424105     0.715733    58.709228   \n",
       "min    -992.795572       1.098476     0.020395    -0.999991  -100.000000   \n",
       "25%    -499.000060   59791.243170     2.388319    -0.760277   -51.000000   \n",
       "50%     -27.174576  241226.533900     2.691213    -0.029038     3.000000   \n",
       "75%     477.216100  547320.664300     2.869121     0.714801    52.000000   \n",
       "max     998.297367  996597.632200     2.999260     0.999992   100.000000   \n",
       "\n",
       "                X6           X7           X8           X9          X10  \\\n",
       "count  1029.000000   1029.00000  1029.000000  1029.000000  1029.000000   \n",
       "mean   -153.782162 -23648.95337    -2.565422     0.018122    -4.108330   \n",
       "std       0.000000      0.00000     0.424105     0.715733   584.648570   \n",
       "min    -153.782162 -23648.95337    -2.999260    -0.999992  -996.652132   \n",
       "25%    -153.782162 -23648.95337    -2.869121    -0.714801  -511.919891   \n",
       "50%    -153.782162 -23648.95337    -2.691213     0.029038    -6.507760   \n",
       "75%    -153.782162 -23648.95337    -2.388319     0.760277   516.046297   \n",
       "max    -153.782162 -23648.95337    -0.020395     0.999991  1000.933758   \n",
       "\n",
       "           ...               X30          X31          X32          X33  \\\n",
       "count      ...       1029.000000  1029.000000  1029.000000  1029.000000   \n",
       "mean       ...          5.612245   100.650146   -29.719145  -136.164237   \n",
       "std        ...        239.009325  2862.331915  4489.803784  2641.944700   \n",
       "min        ...       -410.000000 -4857.000000 -7715.000000 -4596.000000   \n",
       "25%        ...       -189.000000 -2457.000000 -4063.000000 -2438.000000   \n",
       "50%        ...          7.000000   123.000000   129.000000  -196.000000   \n",
       "75%        ...        214.000000  2654.000000  3890.000000  2138.000000   \n",
       "max        ...        410.000000  4862.000000  7752.000000  4577.000000   \n",
       "\n",
       "               X34          X35          X36          X37          X38  \\\n",
       "count  1029.000000  1029.000000  1029.000000  1029.000000  1029.000000   \n",
       "mean   -101.649174   -34.310010    -7.965986   -37.117590     2.499514   \n",
       "std    5153.481524  2569.220416  2933.878621  1504.903744   102.994972   \n",
       "min   -8898.000000 -4383.000000 -5133.000000 -2565.000000  -175.000000   \n",
       "25%   -4780.000000 -2311.000000 -2520.000000 -1411.000000   -85.000000   \n",
       "50%     -13.000000    20.000000   -35.000000    78.000000     6.000000   \n",
       "75%    4195.000000  2165.000000  2521.000000  1227.000000    94.000000   \n",
       "max    8866.000000  4384.000000  5123.000000  2568.000000   175.000000   \n",
       "\n",
       "                  y  \n",
       "count  1.029000e+03  \n",
       "mean  -8.035132e+18  \n",
       "std    2.593868e+20  \n",
       "min   -9.506460e+20  \n",
       "25%   -7.703790e+18  \n",
       "50%   -1.094503e+10  \n",
       "75%    5.636450e+18  \n",
       "max    9.881420e+20  \n",
       "\n",
       "[8 rows x 39 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./regLinPoli2.csv\",header=0)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = df.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase linearRegression\n",
    "\n",
    "Escribí esta clase que implementa una regresión lineal con multiplicaciones matriciales, utiliza el método iterativo y regularización ridge. Con esta es mucho más fácil construir código para probar el funcionamiento de los hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class linearRegression:\n",
    "    \"\"\"Class to implement a vectorized iterative linear regression with ridge regularization.\"\"\"\n",
    "    \n",
    "    def __init__(self, eta=0.01, lmbd=0, iterations=1):\n",
    "        self.ws = np.matrix([]) #empty matrix for the coefficients of our hyperplane\n",
    "        self.eta=eta\n",
    "        self.lmbd=lmbd\n",
    "        self.iterations = iterations\n",
    "    \n",
    "    def add_x0(self,X):\n",
    "        ones = np.asmatrix(np.ones(X.shape[0]))\n",
    "        X_new = np.concatenate((ones.transpose(),X), axis = 1)\n",
    "        return X_new\n",
    "    \n",
    "    def calcEstimation(self,X):\n",
    "        v = X*self.ws.transpose()\n",
    "        return v\n",
    "    \n",
    "    def iterate(self,X,Y):\n",
    "        ws_new = np.asmatrix(np.zeros(self.ws.shape[1]))\n",
    "        \n",
    "        for it in range(self.iterations):\n",
    "            for i in range(X.shape[0]):\n",
    "                error = (Y[0,i] - self.calcEstimation(X[i]))\n",
    "\n",
    "                ws_new[0,0] = (self.ws[0,0] + (self.eta*error[0,0]))\n",
    "                for j in range(1,self.ws.shape[1]):\n",
    "                    ws_new[0,j] = (self.ws[0,j] + (self.eta*error[0,0]*X[i,j]) - (self.lmbd*self.ws[0,j]))\n",
    "            \n",
    "                self.ws = ws_new #update the coefficients.\n",
    "        \n",
    "    \n",
    "    def train(self,X, Y):\n",
    "        #train the ws iteratively.\n",
    "        X = self.add_x0(X) #Add the column of ones\n",
    "        \n",
    "        #Initialize the ws\n",
    "        self.ws = np.asmatrix((np.random.rand(X.shape[1])*10) - 5)\n",
    "        \n",
    "        #iterate through the whole of x.\n",
    "        self.iterate(X,Y)\n",
    "        \n",
    "        #we should be ready to predict\n",
    "    \n",
    "    \n",
    "    def predict(self,X):\n",
    "        #calculate the resulting matrix (vector) and return it.\n",
    "        X = self.add_x0(X)\n",
    "        \n",
    "        return self.calcEstimation(X)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función crossVal\n",
    "\n",
    "Esta función implementa K-fold cross validation con K folds, el dataset entero (del cual la función tomará la última columna como la variable *target*), el valor de lambda y además se le puede especificar si se quiere hacer shuffle del dataset para romper cualquier orden que exista en los datos para que cada fold sea más representativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para hacer K-fold cross validation, shuffle will create a shuffled matrix to break any existing order.\n",
    "#This version assumes the target variable is the last one.\n",
    "def crossVal(k, dataset,lmbd = 0, shuffle=True):\n",
    "    error = 0.0\n",
    "    n = ds.shape[0] #Get the number of tuples\n",
    "    \n",
    "    divTup = n/k #Int division to get the number of tuples in a division\n",
    "    \n",
    "    #shuffle the dataset to make sure any previous order in the dataset is broken.\n",
    "    if(shuffle == True):\n",
    "        np.random.shuffle(dataset)\n",
    "    \n",
    "    #divide the dataset in k folds\n",
    "    foldRanges = np.arange(0,(k+1)*divTup,divTup) # will produce [0,k*divTup] in steps of divTup\n",
    "    \n",
    "    \n",
    "    for i in range(0,k):\n",
    "        #Construct validation set\n",
    "        #construct test set\n",
    "        \n",
    "        leftVal = foldRanges[i]\n",
    "        rightVal = foldRanges[i+1]\n",
    "        \n",
    "        valSet = dataset[leftVal:rightVal,:]\n",
    "        \n",
    "        trainSet1 = dataset[0:leftVal,:]\n",
    "        trainSet2 = dataset[rightVal:,:]\n",
    "        \n",
    "        #join to form the full test set\n",
    "        trainSet = np.concatenate((trainSet1,trainSet2), axis=0)\n",
    "        \n",
    "        X_val = valSet[:,0:-1]\n",
    "        Y_val = valSet[:,-1:]\n",
    "        \n",
    "        X_train = trainSet[:,0:-1]\n",
    "        Y_train = trainSet[:,-1:]\n",
    "        \n",
    "        #a little scaling\n",
    "        \n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train=scaler.transform(X_train)\n",
    "        X_val=scaler.transform(X_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        lr = linearRegression(iterations = 1, eta=0.01,lmbd=lmbd)\n",
    "        lr.train(X_train, Y_train.transpose())\n",
    "        Y_predicted = lr.predict(X_val)\n",
    "\n",
    "        error += mean_squared_error(Y_val, Y_predicted)\n",
    "        \n",
    "        \n",
    "    #Not sure if I would return an average of all the coefficients for the 'final' coefficient answer\n",
    "    \n",
    "    #meanwhile just return the error\n",
    "    return error\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it out!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "density = 8\n",
    "lambdaSpace = np.linspace(0,0.5,density)\n",
    "error = []\n",
    "\n",
    "for i in range(density):\n",
    "    error.append(crossVal(10,ds,lmbd = lambdaSpace[i], shuffle = False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJalBhrskNchwl6QG/R/sZXCbdYA/hgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a206490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lambdaSpace, error, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, parece que tenemos el mismo efecto que en el último trabajo, es decir, que el valor óptimo de lambda es 0.\n",
    "\n",
    "Probemos con un valor diferente para K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nktSg/wNctoBZjwTVuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a212190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "density = 4\n",
    "lambdaSpace = np.linspace(0,0.25,density)\n",
    "error = []\n",
    "\n",
    "for i in range(density):\n",
    "    error.append(crossVal(15,ds,lmbd = lambdaSpace[i], shuffle = False))\n",
    "    \n",
    "plt.plot(lambdaSpace, error, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y pues concluímos que para este caso también el lambda más óptimo es 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "clasemachinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
